{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":120005,"sourceType":"modelInstanceVersion","modelInstanceId":100936,"modelId":121027}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U transformers accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:41:14.425945Z","iopub.execute_input":"2024-11-15T13:41:14.426235Z","iopub.status.idle":"2024-11-15T13:41:26.777145Z","shell.execute_reply.started":"2024-11-15T13:41:14.426197Z","shell.execute_reply":"2024-11-15T13:41:26.775843Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer\nimport torch\n\n\nbase_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    return_dict=True,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:41:26.779895Z","iopub.execute_input":"2024-11-15T13:41:26.780275Z","iopub.status.idle":"2024-11-15T13:42:18.167858Z","shell.execute_reply.started":"2024-11-15T13:41:26.780237Z","shell.execute_reply":"2024-11-15T13:42:18.166792Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94f84312019e46c3a3d96c152e611a45"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"#Setting pad_token_id to avoid receiving messages\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\nif model.config.pad_token_id is None:\n    model.config.pad_token_id = model.config.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:42:18.169106Z","iopub.execute_input":"2024-11-15T13:42:18.169695Z","iopub.status.idle":"2024-11-15T13:42:18.174772Z","shell.execute_reply.started":"2024-11-15T13:42:18.169658Z","shell.execute_reply":"2024-11-15T13:42:18.173881Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:42:18.176662Z","iopub.execute_input":"2024-11-15T13:42:18.177161Z","iopub.status.idle":"2024-11-15T13:42:18.211961Z","shell.execute_reply.started":"2024-11-15T13:42:18.177112Z","shell.execute_reply":"2024-11-15T13:42:18.210911Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"**Beginning fine tune model**","metadata":{"execution":{"iopub.status.busy":"2024-11-15T09:43:43.795905Z","iopub.execute_input":"2024-11-15T09:43:43.796575Z","iopub.status.idle":"2024-11-15T09:43:43.800707Z","shell.execute_reply.started":"2024-11-15T09:43:43.796534Z","shell.execute_reply":"2024-11-15T09:43:43.799804Z"}}},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:42:18.213179Z","iopub.execute_input":"2024-11-15T13:42:18.213489Z","iopub.status.idle":"2024-11-15T13:43:43.514861Z","shell.execute_reply.started":"2024-11-15T13:42:18.213458Z","shell.execute_reply":"2024-11-15T13:43:43.513695Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#Load the Python packages and functions we will use throughout the fine-tuning and evaluation process\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:43:43.516703Z","iopub.execute_input":"2024-11-15T13:43:43.517176Z","iopub.status.idle":"2024-11-15T13:43:45.453411Z","shell.execute_reply.started":"2024-11-15T13:43:43.517113Z","shell.execute_reply":"2024-11-15T13:43:45.452550Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from huggingface_hub import login\n\n\nhf_token = \"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\"\nlogin(token = hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T14:55:12.513941Z","iopub.execute_input":"2024-11-15T14:55:12.514653Z","iopub.status.idle":"2024-11-15T14:55:12.603782Z","shell.execute_reply.started":"2024-11-15T14:55:12.514612Z","shell.execute_reply":"2024-11-15T14:55:12.602880Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# wandb api key - c1654cbca8f17919e30147f14e109f52673945e9\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:43:45.585690Z","iopub.execute_input":"2024-11-15T13:43:45.586015Z","iopub.status.idle":"2024-11-15T13:43:45.590277Z","shell.execute_reply.started":"2024-11-15T13:43:45.585982Z","shell.execute_reply":"2024-11-15T13:43:45.589328Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"wb_token = \"c1654cbca8f17919e30147f14e109f52673945e9\"\n\nwandb.login(key=wb_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:43:45.591546Z","iopub.execute_input":"2024-11-15T13:43:45.591940Z","iopub.status.idle":"2024-11-15T13:43:46.942241Z","shell.execute_reply.started":"2024-11-15T13:43:45.591895Z","shell.execute_reply":"2024-11-15T13:43:46.941185Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# PROJECT name- Fine-tune-LLAMA3.2\nrun = wandb.init(\n    project='Fine-tune-LLAMA3.2', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:43:46.943626Z","iopub.execute_input":"2024-11-15T13:43:46.944352Z","iopub.status.idle":"2024-11-15T13:43:48.717201Z","shell.execute_reply.started":"2024-11-15T13:43:46.944315Z","shell.execute_reply":"2024-11-15T13:43:48.716192Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdyssjsnke\u001b[0m (\u001b[33mdyssjsnke-panjab-univeristy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241115_134346-qflwow4g</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dyssjsnke-panjab-univeristy/Fine-tune-llama3.2/runs/qflwow4g' target=\"_blank\">usual-elevator-4</a></strong> to <a href='https://wandb.ai/dyssjsnke-panjab-univeristy/Fine-tune-llama3.2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dyssjsnke-panjab-univeristy/Fine-tune-llama3.2' target=\"_blank\">https://wandb.ai/dyssjsnke-panjab-univeristy/Fine-tune-llama3.2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dyssjsnke-panjab-univeristy/Fine-tune-llama3.2/runs/qflwow4g' target=\"_blank\">https://wandb.ai/dyssjsnke-panjab-univeristy/Fine-tune-llama3.2/runs/qflwow4g</a>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\nnew_model = \"llama-3.2-3b-uj712\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:43:48.718682Z","iopub.execute_input":"2024-11-15T13:43:48.719360Z","iopub.status.idle":"2024-11-15T13:43:48.724553Z","shell.execute_reply.started":"2024-11-15T13:43:48.719313Z","shell.execute_reply":"2024-11-15T13:43:48.723453Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"dataset_name = load_dataset(\"Ujjwal671021/jac-chandigarh-information-brochure\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:43:48.725649Z","iopub.execute_input":"2024-11-15T13:43:48.725956Z","iopub.status.idle":"2024-11-15T13:43:51.638769Z","shell.execute_reply.started":"2024-11-15T13:43:48.725925Z","shell.execute_reply":"2024-11-15T13:43:51.637900Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/368 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01a52438f25d48d58f38aea0e115db0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/145k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5a84785672647bea2f1c408d855acfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/48.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4cddd760f4042ea87b37a0da940018c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b18cbae1813418595ee1740c44b5c1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/422 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e76aee5d53a4a9c9291b71c8aed3405"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"dataset_name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:43:51.640121Z","iopub.execute_input":"2024-11-15T13:43:51.640433Z","iopub.status.idle":"2024-11-15T13:43:51.647312Z","shell.execute_reply.started":"2024-11-15T13:43:51.640392Z","shell.execute_reply":"2024-11-15T13:43:51.646271Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 1300\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 422\n    })\n})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Set torch dtype and attention implementation\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install -qqq flash-attn\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"flash_attention_2\"\nelse:\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:43:51.648503Z","iopub.execute_input":"2024-11-15T13:43:51.648795Z","iopub.status.idle":"2024-11-15T13:43:51.662451Z","shell.execute_reply.started":"2024-11-15T13:43:51.648763Z","shell.execute_reply":"2024-11-15T13:43:51.661374Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:43:51.663797Z","iopub.execute_input":"2024-11-15T13:43:51.664465Z","iopub.status.idle":"2024-11-15T13:43:59.856186Z","shell.execute_reply.started":"2024-11-15T13:43:51.664417Z","shell.execute_reply":"2024-11-15T13:43:59.855376Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e7e139700e44e2cbc2e0d4d17d172ad"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# %pip install -U bitsandbytes\ndataset_name\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:44:00.351712Z","iopub.execute_input":"2024-11-15T13:44:00.352601Z","iopub.status.idle":"2024-11-15T13:44:00.360599Z","shell.execute_reply.started":"2024-11-15T13:44:00.352563Z","shell.execute_reply":"2024-11-15T13:44:00.359646Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 1300\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 422\n    })\n})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"instruction = \"You are a helpful assistant. Format the following text for chat.\"\n\ndef format_chat_template(row):\n    row_json = [\n        {\"role\": \"system\", \"content\": instruction},\n        {\"role\": \"user\", \"content\": row[\"text\"]},  # Assuming \"text\" field exists in the dataset\n        {\"role\": \"assistant\", \"content\": \"Your response here.\"}  # Replace this with an appropriate response logic\n    ]\n    \n    # Apply the chat template to the \"text\" field\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\n# Process the dataset with multiprocessing\ndataset_name = dataset_name.map(\n    format_chat_template,\n    num_proc=4,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T13:58:56.929291Z","iopub.execute_input":"2024-11-15T13:58:56.930147Z","iopub.status.idle":"2024-11-15T13:58:58.479864Z","shell.execute_reply.started":"2024-11-15T13:58:56.930103Z","shell.execute_reply":"2024-11-15T13:58:58.478903Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d30d061e6344cc2aec54d902c993851"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/422 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9249698af2194f27b88a66caa4717a86"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"torch_dtype = torch.float16\nattn_implementation = \"eager\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T14:01:15.629167Z","iopub.execute_input":"2024-11-15T14:01:15.629580Z","iopub.status.idle":"2024-11-15T14:01:15.635388Z","shell.execute_reply.started":"2024-11-15T14:01:15.629538Z","shell.execute_reply":"2024-11-15T14:01:15.634510Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T14:01:23.485401Z","iopub.execute_input":"2024-11-15T14:01:23.485808Z","iopub.status.idle":"2024-11-15T14:01:31.343274Z","shell.execute_reply.started":"2024-11-15T14:01:23.485771Z","shell.execute_reply":"2024-11-15T14:01:31.342415Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8351ec65ed94ba085f7a4fe821b0cfa"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\n# Reset chat template to None if it already exists\nif hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template is not None:\n    tokenizer.chat_template = None  # Reset the chat template\n\n# Set up the model and tokenizer with the desired chat format\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T14:04:41.388824Z","iopub.execute_input":"2024-11-15T14:04:41.389616Z","iopub.status.idle":"2024-11-15T14:04:49.275697Z","shell.execute_reply.started":"2024-11-15T14:04:41.389574Z","shell.execute_reply":"2024-11-15T14:04:49.274452Z"}},"outputs":[{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"dataset_name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T14:17:41.961630Z","iopub.execute_input":"2024-11-15T14:17:41.962384Z","iopub.status.idle":"2024-11-15T14:17:42.132142Z","shell.execute_reply.started":"2024-11-15T14:17:41.962332Z","shell.execute_reply":"2024-11-15T14:17:42.130884Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset_name\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:72\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     75\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     76\u001b[0m         ]\n","\u001b[0;31mKeyError\u001b[0m: 'text'"],"ename":"KeyError","evalue":"'text'","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"import bitsandbytes as bnb\n\ndef find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:  # needed for 16 bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\nmodules = find_all_linear_names(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T14:19:46.949968Z","iopub.execute_input":"2024-11-15T14:19:46.950720Z","iopub.status.idle":"2024-11-15T14:19:46.959808Z","shell.execute_reply.started":"2024-11-15T14:19:46.950678Z","shell.execute_reply":"2024-11-15T14:19:46.958910Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Define LoRA configuration\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=modules,\n)\n\n# Reset chat_template if it exists\nif hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template is not None:\n    tokenizer.chat_template = None  # Reset the chat template\n\n# Set up the chat format\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n\n# Apply LoRA configuration\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T14:24:38.199246Z","iopub.execute_input":"2024-11-15T14:24:38.200093Z","iopub.status.idle":"2024-11-15T14:24:38.804547Z","shell.execute_reply.started":"2024-11-15T14:24:38.200053Z","shell.execute_reply":"2024-11-15T14:24:38.803479Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"#Hyperparamter\ntraining_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T14:28:29.193204Z","iopub.execute_input":"2024-11-15T14:28:29.194173Z","iopub.status.idle":"2024-11-15T14:28:29.233766Z","shell.execute_reply.started":"2024-11-15T14:28:29.194116Z","shell.execute_reply":"2024-11-15T14:28:29.232980Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Setting sft parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset_name[\"train\"],\n    eval_dataset=dataset_name[\"test\"],\n    peft_config=peft_config,\n    max_seq_length= 512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T14:28:52.035208Z","iopub.execute_input":"2024-11-15T14:28:52.035977Z","iopub.status.idle":"2024-11-15T14:28:53.314378Z","shell.execute_reply.started":"2024-11-15T14:28:52.035938Z","shell.execute_reply":"2024-11-15T14:28:53.313562Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"286c99d3bf9a4ba89c2ebedc56fbda24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/422 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c072daf813404a6e92dca36398bb09b6"}},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T14:29:27.242114Z","iopub.execute_input":"2024-11-15T14:29:27.242553Z","iopub.status.idle":"2024-11-15T14:44:04.229277Z","shell.execute_reply.started":"2024-11-15T14:29:27.242514Z","shell.execute_reply":"2024-11-15T14:44:04.228373Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='650' max='650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [650/650 14:34, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>130</td>\n      <td>0.822600</td>\n      <td>0.969285</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.097600</td>\n      <td>0.934418</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.493600</td>\n      <td>0.914597</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.845900</td>\n      <td>0.897863</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.272900</td>\n      <td>0.889605</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=650, training_loss=0.9297352568919842, metrics={'train_runtime': 876.1505, 'train_samples_per_second': 1.484, 'train_steps_per_second': 0.742, 'total_flos': 1988118613991424.0, 'train_loss': 0.9297352568919842, 'epoch': 1.0})"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T14:44:49.214821Z","iopub.execute_input":"2024-11-15T14:44:49.215296Z","iopub.status.idle":"2024-11-15T14:44:50.767194Z","shell.execute_reply.started":"2024-11-15T14:44:49.215256Z","shell.execute_reply":"2024-11-15T14:44:50.766293Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <style>\n        .wandb-row {\n            display: flex;\n            flex-direction: row;\n            flex-wrap: wrap;\n            justify-content: flex-start;\n            width: 100%;\n        }\n        .wandb-col {\n            display: flex;\n            flex-direction: column;\n            flex-basis: 100%;\n            flex: 1;\n            padding: 10px;\n        }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▃▂▁</td></tr><tr><td>eval/runtime</td><td>█▇▂▁▃</td></tr><tr><td>eval/samples_per_second</td><td>▁▂▇█▆</td></tr><tr><td>eval/steps_per_second</td><td>▁▂▇█▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▆▇█▁▂▃▂▃▇▁▃▃▂▃▃▃▅▃▃▄▄▃▃▃▄▂▁▂▂▂▃▁▂▂▃▃▁▅▄▂</td></tr><tr><td>train/learning_rate</td><td>▂▃███▇▇▇▇▇▇▇▆▆▆▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▂▃▁▃▂▂▂▂▂▂▃▂▃▂▂▂▂▁▂▃▂▃▃▃▃▁▁▂▁▂▃▃▂▁▃▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.8896</td></tr><tr><td>eval/runtime</td><td>67.3476</td></tr><tr><td>eval/samples_per_second</td><td>6.266</td></tr><tr><td>eval/steps_per_second</td><td>6.266</td></tr><tr><td>total_flos</td><td>1988118613991424.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>650</td></tr><tr><td>train/grad_norm</td><td>0.62568</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.2729</td></tr><tr><td>train_loss</td><td>0.92974</td></tr><tr><td>train_runtime</td><td>876.1505</td></tr><tr><td>train_samples_per_second</td><td>1.484</td></tr><tr><td>train_steps_per_second</td><td>0.742</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">usual-elevator-4</strong> at: <a href='https://wandb.ai/dyssjsnke-panjab-univeristy/Fine-tune-llama3.2/runs/qflwow4g' target=\"_blank\">https://wandb.ai/dyssjsnke-panjab-univeristy/Fine-tune-llama3.2/runs/qflwow4g</a><br/> View project at: <a href='https://wandb.ai/dyssjsnke-panjab-univeristy/Fine-tune-llama3.2' target=\"_blank\">https://wandb.ai/dyssjsnke-panjab-univeristy/Fine-tune-llama3.2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241115_134346-qflwow4g/logs</code>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"messages = [{\"role\": \"system\", \"content\": instruction},\n    {\"role\": \"user\", \"content\": \"What is full form of CCET College\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T14:47:05.499722Z","iopub.execute_input":"2024-11-15T14:47:05.500730Z","iopub.status.idle":"2024-11-15T14:47:27.016445Z","shell.execute_reply.started":"2024-11-15T14:47:05.500684Z","shell.execute_reply":"2024-11-15T14:47:27.015414Z"}},"outputs":[{"name":"stdout","text":". Format the following text for chat.\nuser\nWhat is full form of CCET College\n\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T14:55:17.594393Z","iopub.execute_input":"2024-11-15T14:55:17.595233Z","iopub.status.idle":"2024-11-15T14:56:18.567065Z","shell.execute_reply.started":"2024-11-15T14:55:17.595192Z","shell.execute_reply":"2024-11-15T14:56:18.566124Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/1.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6945550a29844d05aed8e0c30e81395f"}},"metadata":{}},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Ujjwal671021/llama-3.2-3b-uj712/commit/c9fc6834a3b34be149391f128dae13e4020017e5', commit_message='Upload model', commit_description='', oid='c9fc6834a3b34be149391f128dae13e4020017e5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Ujjwal671021/llama-3.2-3b-uj712', endpoint='https://huggingface.co', repo_type='model', repo_id='Ujjwal671021/llama-3.2-3b-uj712'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}